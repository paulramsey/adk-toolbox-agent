{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321adc7f-f386-4dbf-bb9c-d5f789421f78",
   "metadata": {},
   "source": [
    "# Configure AlloyDB AI Natural Language\n",
    "\n",
    "> IMPORTANT: This is the fourth notebook in the lab. The notebooks build on top of each other, so be sure to run the preceding notebooks, in order, before running this one. Start your journey building ADK Agents with MCP Toolbox [here](./1_setup_and_explore_databases.ipynb). \n",
    "\n",
    "> IMPORTANT: At the time of writing, AlloyDB Natural Language is a Private Preview feature. Request access to the feature [using this form](https://docs.google.com/forms/d/e/1FAIpQLSfJ9vHIJ79nI7JWBDELPFL75pDQa4XVZQ2fxShfYddW0RwmLw/viewform) before executing this notebook.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook introduces you to AlloyDB AI Natural Language, a powerful new way to interact with databases. You will learn how to integrate this feature into your database and leverage it as an MCP Toolbox tool to build a more intuitive and flexible ADK agent. The key milestones you will achieve are:\n",
    "\n",
    "- Configuring AlloyDB AI Natural Language: You will walk through the process of setting up AlloyDB AI's natural language capabilities on your finance database. This includes enabling the necessary extensions, creating a natural language configuration, registering tables, and generating context to help the model understand your schema.\n",
    "- Deploying an Advanced Tool to MCP Toolbox: You will create and deploy an MCP Toolbox tool that allows your agent to translate natural language questions into SQL queries, enabling more dynamic and flexible interactions with the database.\n",
    "- Integrating the Natural Language Tool with the ADK Agent: You will update your deployed ADK agent to use the new natural language tool. This will demonstrate how to build an agent that can understand and respond to a wide range of user queries without needing predefined, specific tools for every possible question.\n",
    "- Testing the Enhanced Agent: You will interact with the updated agent to see the AlloyDB natural language feature in action. This will highlight how the agent can now handle more complex and varied questions about your financial data, showcasing the power of combining ADK, MCP Toolbox, and AlloyDB AI.\n",
    "\n",
    "### Terraform Resources\n",
    "\n",
    "The following pre-requisite resources were created for you by Terraform. See the [main.tf](../terraform/main.tf) file for more details on the environment configuration:\n",
    "\n",
    "- AlloyDB: The AlloyDB cluster and instance that you will configure with the natural language feature.\n",
    "- Custom VPC (demo-vpc): The private network that ensures secure communication between the ADK agent, MCP Toolbox, and the AlloyDB instance.\n",
    "- Cloud Run: Hosts the ADK agent and MCP Toolbox as secure, scalable services within a private VPC.\n",
    "- Vertex AI Workbench: The Vertex AI Workbench instance where you are executing this notebook.\n",
    "\n",
    "### Google Cloud Services Used\n",
    "\n",
    "This notebook utilizes the following Google Cloud services:\n",
    "- AlloyDB for PostgreSQL: The core database service that you will enhance with the AI natural language feature.\n",
    "- Vertex AI: The platform that provides the underlying AI and machine learning capabilities for the AlloyDB natural language feature.\n",
    "- MCP Toolbox for Databases: The open-source tool server that you will update with the new natural language tool.\n",
    "- Cloud Run: The service that hosts your deployed ADK agent and MCP Toolbox.\n",
    "- Secret Manager: Used to securely manage your tools.yaml configuration and database credentials.\n",
    "- IAM (Identity and Access Management): Manages the necessary permissions for the services and service accounts to interact with each other.\n",
    "\n",
    "### Logical Flow\n",
    "\n",
    "This notebook is structured to guide you step-by-step through the process of enabling and using the AlloyDB natural language feature:\n",
    "- Basic Setup: This section prepares your environment by defining variables, connecting to your Google Cloud project, and installing the required Python libraries.\n",
    "- Integrate AlloyDB with Vertex AI: You will connect to your AlloyDB cluster and grant the necessary permissions for it to integrate with Vertex AI, which is a prerequisite for using the natural language feature.\n",
    "- Natural Language Setup: This is the core of the notebook, where you will:\n",
    "    - Enable the alloydb_ai_nl extension.\n",
    "    - Create a natural language configuration.\n",
    "    - Register the tables from your finance database with the configuration.\n",
    "    - Generate and apply context for your tables and columns, which helps the AI model understand your schema.\n",
    "    - Define query templates to provide the model with examples of valid SQL.\n",
    "- Generate and Execute SQL with AlloyDB Natural Language: You will test the natural language feature directly in the database by providing natural language questions and observing the generated SQL.\n",
    "- Use AlloyDB Natural Language with MCP Toolbox: You will update your `tools.yaml` file to include a new tool of kind alloydb-ai-nl, which will allow your agent to leverage the natural language capabilities you just configured.\n",
    "- Use AlloyDB Natural Language with the ADK Agent: Finally, you will update your deployed ADK agent to use the new tool and interact with it to ask a variety of natural language questions about your financial data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927ae02-125e-4d3e-a885-1e437e6bddb6",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842a3ca3-4f62-4190-bbb6-10e7da9b13c8",
   "metadata": {},
   "source": [
    "### Define Notebook Variables\n",
    "\n",
    "Update the variables below to match your environment. You will be prompted for the AlloyDB password you chose then you provisioned the environment with Terraform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa431b-2efd-4db1-89e1-b97b4fc722df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project variables\n",
    "project_id = \"your-project\"\n",
    "region = \"your-region\"\n",
    "vpc = \"demo-vpc\"\n",
    "gcs_bucket_name = f\"project-files-{project_id}\"\n",
    "\n",
    "# AlloyDB variables\n",
    "alloydb_cluster = \"my-alloydb-cluster\"\n",
    "alloydb_instance = \"my-alloydb-instance\"\n",
    "alloydb_database = \"finance\"\n",
    "alloydb_password = input(\"Please enter the password for the AlloyDB 'postgres' database user: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db8a8b-cae4-4978-bb9a-e2e5a8465696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set env variable to suppress annoying system warnings when running shell commands\n",
    "%env GRPC_ENABLE_FORK_SUPPORT=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc39dc-f6ee-455e-853a-643c12918c2f",
   "metadata": {},
   "source": [
    "### Connect to your Google Cloud Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d972baa-ffa7-47c7-a965-fdaa114262b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure gcloud.\n",
    "!gcloud config set project {project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a124be-854e-4465-bab2-44ea855d9256",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66056e25-5e3a-47e3-a842-8fe6fadd9a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure the root logger to output messages with INFO level or above\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='%(asctime)s[%(levelname)5s][%(name)14s] - %(message)s',  datefmt='%H:%M:%S', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72eb606-f634-424f-be79-c1b75b1b1e14",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2e387-9dfc-49c2-8a32-2c54d7eebcc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --quiet google-cloud-storage==2.19.0 \\\n",
    "                      asyncpg==0.30.0 \\\n",
    "                      google.cloud.alloydb.connector==1.9.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5cf49a-a04d-429c-a5f3-2df8e068fc81",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349aa29e-ccbb-4aef-b51b-20d9130c1e53",
   "metadata": {},
   "source": [
    "#### REST API Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0ad2b-fed0-434f-baa7-69247ee476c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import google.auth\n",
    "import json\n",
    "\n",
    "# Get an access token based upon the current user\n",
    "creds, _ = google.auth.default()\n",
    "authed_session = google.auth.transport.requests.AuthorizedSession(creds)\n",
    "access_token=creds.token\n",
    "\n",
    "if project_id:\n",
    "  authed_session.headers.update({\"x-goog-user-project\": project_id}) # Required to workaround a project quota bug\n",
    "\n",
    "def rest_api_helper(\n",
    "    url: str,\n",
    "    http_verb: str,\n",
    "    request_body: dict = None,\n",
    "    params: dict = None,\n",
    "    session: requests.Session = authed_session,\n",
    "  ) -> dict:\n",
    "  \"\"\"Calls a REST API using a pre-authenticated requests Session.\"\"\"\n",
    "\n",
    "  headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "  try:\n",
    "\n",
    "    if http_verb == \"GET\":\n",
    "      response = session.get(url, headers=headers, params=params)\n",
    "    elif http_verb == \"POST\":\n",
    "      response = session.post(url, json=request_body, headers=headers, params=params)\n",
    "    elif http_verb == \"PUT\":\n",
    "      response = session.put(url, json=request_body, headers=headers, params=params)\n",
    "    elif http_verb == \"PATCH\":\n",
    "      response = session.patch(url, json=request_body, headers=headers, params=params)\n",
    "    elif http_verb == \"DELETE\":\n",
    "      response = session.delete(url, headers=headers, params=params)\n",
    "    else:\n",
    "      raise ValueError(f\"Unknown HTTP verb: {http_verb}\")\n",
    "\n",
    "    # Raise an exception for bad status codes (4xx or 5xx)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Check if response has content before trying to parse JSON\n",
    "    if response.content:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {} # Return empty dict for empty responses (like 204 No Content)\n",
    "\n",
    "  except requests.exceptions.RequestException as e:\n",
    "      # Catch potential requests library errors (network, timeout, etc.)\n",
    "      # Log detailed error information\n",
    "      print(f\"Request failed: {e}\")\n",
    "      if e.response is not None:\n",
    "          print(f\"Request URL: {e.request.url}\")\n",
    "          print(f\"Request Headers: {e.request.headers}\")\n",
    "          print(f\"Request Body: {e.request.body}\")\n",
    "          print(f\"Response Status: {e.response.status_code}\")\n",
    "          print(f\"Response Text: {e.response.text}\")\n",
    "          # Re-raise a more specific error or a custom one\n",
    "          raise RuntimeError(f\"API call failed with status {e.response.status_code}: {e.response.text}\") from e\n",
    "      else:\n",
    "          raise RuntimeError(f\"API call failed: {e}\") from e\n",
    "  except json.JSONDecodeError as e:\n",
    "      print(f\"Failed to decode JSON response: {e}\")\n",
    "      print(f\"Response Text: {response.text}\")\n",
    "      raise RuntimeError(f\"Invalid JSON received from API: {response.text}\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f4658-e4c5-4c3f-badf-da3f3d6cdd3a",
   "metadata": {},
   "source": [
    "#### AlloyDB Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b976a5-7b18-41d2-89c9-dbd2b9a2aa45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create AlloyDB Query Helper Function\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text, exc\n",
    "import pandas as pd\n",
    "\n",
    "async def run_alloydb_query(pool, sql: str, params = None, output_as_df: bool = True):\n",
    "    \"\"\"Executes a SQL query or statement against the database pool.\n",
    "\n",
    "    Handles various SQL statements:\n",
    "    - SELECT/WITH: Returns results as a DataFrame (if output_as_df=True)\n",
    "      or ResultProxy. Supports parameters. Does not commit.\n",
    "    - EXPLAIN/EXPLAIN ANALYZE: Executes the explain, returns the query plan\n",
    "      as a formatted multi-line string. Ignores output_as_df.\n",
    "      Supports parameters. Does not commit.\n",
    "    - INSERT/UPDATE/DELETE/CREATE/ALTER etc.: Executes the statement,\n",
    "      commits the transaction, logs info, and returns the ResultProxy.\n",
    "      Supports single or bulk parameters (executemany).\n",
    "\n",
    "    Args:\n",
    "      pool: An asynchronous SQLAlchemy connection pool.\n",
    "      sql: A string containing the SQL query or statement template.\n",
    "      params: Optional.\n",
    "        - None: Execute raw SQL (Use with caution for non-SELECT/EXPLAIN).\n",
    "        - dict or tuple: Parameters for a single execution.\n",
    "        - list of dicts/tuples: Parameters for bulk execution (executemany).\n",
    "      output_as_df (bool): If True and query is SELECT/WITH, return pandas DataFrame.\n",
    "                           Ignored for EXPLAIN and non-data-returning statements.\n",
    "\n",
    "    Returns:\n",
    "      pandas.DataFrame | str | sqlalchemy.engine.Result | None:\n",
    "        - DataFrame: For SELECT/WITH if output_as_df=True.\n",
    "        - str: For EXPLAIN/EXPLAIN ANALYZE, containing the formatted query plan.\n",
    "        - ResultProxy: For non-SELECT/WITH/EXPLAIN statements, or SELECT/WITH\n",
    "                       if output_as_df=False.\n",
    "        - None: If a SQLAlchemy ProgrammingError or other specific error occurs.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Catches and logs `sqlalchemy.exc.ProgrammingError`, returning None.\n",
    "                   May re-raise other database exceptions.\n",
    "\n",
    "    Example Execution:\n",
    "      Single SELECT:\n",
    "        sql_select = \"SELECT ticker, company_name from investments LIMIT 5\"\n",
    "        df_result = await run_alloydb_query(pool, sql_select)\n",
    "\n",
    "      Single non-SELECT - Parameterized (Safe!):\n",
    "        Parameterized INSERT:\n",
    "          sql_insert = \"INSERT INTO investments (ticker, company_name) VALUES (:ticker, :name)\"\n",
    "          params_insert = {\"ticker\": \"NEW\", \"name\": \"New Company\"}\n",
    "          insert_result = await run_alloydb_query(pool, sql_insert, params_insert)\n",
    "\n",
    "        Parameterized UPDATE:\n",
    "          sql_update = \"UPDATE products SET price = :price WHERE id = :product_id\"\n",
    "          params_update = {\"price\": 99.99, \"product_id\": 123}\n",
    "          update_result = await run_alloydb_query(pool, sql_update, params_update)\n",
    "\n",
    "      Bulk Update:\n",
    "        docs = pd.DataFrame([\n",
    "            {'id': 101, 'sparse_embedding': '[0.1, 0.2]'},\n",
    "            {'id': 102, 'sparse_embedding': '[0.3, 0.4]'},\n",
    "            # ... more rows\n",
    "        ])\n",
    "\n",
    "        update_sql_template = '''\n",
    "            UPDATE products\n",
    "            SET sparse_embedding = :embedding,\n",
    "                sparse_embedding_model = 'BM25'\n",
    "            WHERE id = :product_id\n",
    "        ''' # Using named parameters :param_name\n",
    "\n",
    "        # Prepare list of dictionaries for params\n",
    "        data_to_update = [\n",
    "            {\"embedding\": row.sparse_embedding, \"product_id\": row.id}\n",
    "            for row in docs.itertuples(index=False)\n",
    "        ]\n",
    "\n",
    "        if data_to_update:\n",
    "          bulk_result = await run_alloydb_query(pool, update_sql_template, data_to_update)\n",
    "          # bulk_result is the SQLAlchemy ResultProxy\n",
    "\n",
    "    \"\"\"\n",
    "    sql_lower_stripped = sql.strip().lower()\n",
    "    is_select_with = sql_lower_stripped.startswith(('select', 'with'))\n",
    "    is_explain = sql_lower_stripped.startswith('explain')\n",
    "\n",
    "    # Determine if the statement is expected to return data rows or a plan\n",
    "    is_data_returning = is_select_with or is_explain\n",
    "\n",
    "    # Determine actual DataFrame output eligibility (only for SELECT/WITH)\n",
    "    effective_output_as_df = output_as_df and is_select_with\n",
    "\n",
    "    # Check if params suggest a bulk operation (for logging purposes)\n",
    "    is_bulk_operation = isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], (dict, tuple, list))\n",
    "\n",
    "    async with pool.connect() as conn:\n",
    "        try:\n",
    "          # Execute with or without params\n",
    "          if params:\n",
    "              result = await conn.execute(text(sql), params)\n",
    "          else:\n",
    "              # Add warning for raw SQL only if it's NOT data-returning\n",
    "              #if not is_data_returning:\n",
    "                  #logging.warning(\"Executing non-SELECT/EXPLAIN raw SQL without parameters. Ensure SQL is safe.\")\n",
    "              result = await conn.execute(text(sql))\n",
    "\n",
    "          # --- Handle statements that return data or plan ---\n",
    "          if is_data_returning:\n",
    "              if is_explain:\n",
    "                  # Fetch and format EXPLAIN output as a string\n",
    "                    try:\n",
    "                        plan_rows = result.fetchall()\n",
    "                        # EXPLAIN output is usually text in the first column\n",
    "                        query_plan = \"\\n\".join([str(row[0]) for row in plan_rows])\n",
    "                        #logging.info(f\"EXPLAIN executed successfully for: {sql[:100]}...\")\n",
    "                        return query_plan\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error fetching/formatting EXPLAIN result: {e}\")\n",
    "                        return None\n",
    "              else: # Handle SELECT / WITH\n",
    "                  if effective_output_as_df:\n",
    "                      try:\n",
    "                          rows = result.fetchall()\n",
    "                          column_names = result.keys()\n",
    "                          df = pd.DataFrame(rows, columns=column_names)\n",
    "                          #logging.info(f\"SELECT/WITH executed successfully, returning DataFrame for: {sql[:100]}...\")\n",
    "                          return df\n",
    "                      except Exception as e:\n",
    "                          logging.error(f\"Error converting SELECT result to DataFrame: {e}\")\n",
    "                          logging.info(f\"Returning raw ResultProxy for SELECT/WITH due to DataFrame conversion error for: {sql[:100]}...\")\n",
    "                          return result # Fallback to raw result\n",
    "                  else:\n",
    "                      # Return raw result proxy for SELECT/WITH if df output not requested\n",
    "                      #logging.info(f\"SELECT/WITH executed successfully, returning ResultProxy for: {sql[:100]}...\")\n",
    "                      return result\n",
    "\n",
    "          # --- Handle Non-Data Returning Statements (INSERT, UPDATE, DELETE, CREATE, etc.) ---\n",
    "          else:\n",
    "              await conn.commit() # Commit changes ONLY for these statements\n",
    "              operation_type = sql.strip().split()[0].upper()\n",
    "              row_count = result.rowcount # Note: rowcount behavior varies\n",
    "\n",
    "              if is_bulk_operation:\n",
    "                  print(f\"Bulk {operation_type} executed for {len(params)} items. Result rowcount: {row_count}\")\n",
    "              elif operation_type in ['INSERT', 'UPDATE', 'DELETE']:\n",
    "                  print(f\"{operation_type} statement executed successfully. {row_count} row(s) affected.\")\n",
    "              else: # CREATE, ALTER, etc.\n",
    "                  print(f\"{operation_type} statement executed successfully. Result rowcount: {row_count}\")\n",
    "              return result # Return the result proxy\n",
    "\n",
    "        except exc.ProgrammingError as e:\n",
    "            # Log the error with context\n",
    "            logging.error(f\"SQL Programming Error executing query:\\nSQL: {sql[:500]}...\\nParams (sample): {str(params)[:500]}...\\nError: {e}\")\n",
    "            # Rollback might happen automatically on context exit with error, but explicit can be clearer\n",
    "            # await conn.rollback() # Consider if needed based on pool/transaction settings\n",
    "            return None # Return None on handled programming errors\n",
    "        except Exception as e:\n",
    "            # Log other unexpected errors\n",
    "            logging.error(f\"An unexpected error occurred during query execution:\\nSQL: {sql[:500]}...\\nError: {e}\")\n",
    "            # await conn.rollback() # Consider if needed\n",
    "            raise # Re-raise unexpected errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c9f67-ab0a-419f-bb2e-2ccb3b94be33",
   "metadata": {},
   "source": [
    "## Integrate AlloyDB with Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989a7a1-7fd3-4fa4-9782-a69029594a71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to the AlloyDB Cluster\n",
    "\n",
    "This function will create a connection pool to your AlloyDB instance using the AlloyDB Python connector. The AlloyDB Python connector will automatically create secure connections to your AlloyDB instance using mTLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8c550-32a1-4bab-b1ed-86b85ea92252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncpg\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.asyncio import AsyncEngine, create_async_engine\n",
    "\n",
    "from google.cloud.alloydb.connector import AsyncConnector, IPTypes\n",
    "\n",
    "async def init_connection_pool(connector: AsyncConnector, db_name: str = alloydb_database, pool_size: int = 5) -> AsyncEngine:\n",
    "    # initialize Connector object for connections to AlloyDB\n",
    "    connection_string = f\"projects/{project_id}/locations/{region}/clusters/{alloydb_cluster}/instances/{alloydb_instance}\"\n",
    "\n",
    "    async def getconn() -> asyncpg.Connection:\n",
    "        conn: asyncpg.Connection = await connector.connect(\n",
    "            connection_string,\n",
    "            \"asyncpg\",\n",
    "            user=\"postgres\",\n",
    "            password=alloydb_password,\n",
    "            db=db_name,\n",
    "            ip_type=IPTypes.PRIVATE, # Optionally use IPTypes.PUBLIC\n",
    "        )\n",
    "        return conn\n",
    "\n",
    "    pool = create_async_engine(\n",
    "        \"postgresql+asyncpg://\",\n",
    "        async_creator=getconn,\n",
    "        pool_size=pool_size,\n",
    "        max_overflow=0,\n",
    "        isolation_level='AUTOCOMMIT'\n",
    "    )\n",
    "    return pool\n",
    "\n",
    "connector = AsyncConnector()\n",
    "\n",
    "finance_db_pool = await init_connection_pool(connector, f\"{alloydb_database}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4428954-f1cd-4531-8593-176622679975",
   "metadata": {},
   "source": [
    "### Grant Vertex AI Permission to AlloyDB Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9710f6f0-e620-4a04-bf2e-a7d62251004a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get project number\n",
    "project_number = ! gcloud projects describe {project_id} --format='value(projectNumber)'\n",
    "project_number = project_number[0]\n",
    "\n",
    "# Add Vertex AI Permissions\n",
    "alloydb_service_account = f\"serviceAccount:service-{project_number}@gcp-sa-alloydb.iam.gserviceaccount.com\"\n",
    "!gcloud projects add-iam-policy-binding {project_id} \\\n",
    "    --member=\"{alloydb_service_account}\" \\\n",
    "    --role=\"roles/aiplatform.user\"\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c2e09-083f-445b-8ae5-3eaeb19ead2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create `google_ml_integration` Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5ec04-01dc-4bb3-90a1-07cd966b5d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql = \"CREATE EXTENSION IF NOT EXISTS google_ml_integration;\"\n",
    "await run_alloydb_query(finance_db_pool, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db82f8-af59-42c3-93c4-11d80f13ed42",
   "metadata": {},
   "source": [
    "## Natural Language Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b46ab-243f-4693-a86d-0090c4c4cdeb",
   "metadata": {},
   "source": [
    "### Create `alloydb_ai_nl` Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4e06f-4bc6-45da-84ae-9c58f7611c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql = \"CREATE EXTENSION IF NOT EXISTS alloydb_ai_nl cascade;\"\n",
    "await run_alloydb_query(finance_db_pool, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdcf4a-a326-4f3e-ae29-654381d0ad90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the extension is installed\n",
    "sql = \"SELECT extversion FROM pg_extension WHERE extname = 'alloydb_ai_nl';\"\n",
    "await run_alloydb_query(finance_db_pool, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3affa08c-8043-46cb-88aa-17565d4344a7",
   "metadata": {},
   "source": [
    "### Create a Natural Language Configuration\n",
    "\n",
    "This creates a configuration that will group together the tables, context, and templates for our natural language agent. Success will be shown as `g_create_configuration = None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece49c15-8477-46e9-8985-4d9ee14431b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nl_config = \"finance_agent_config\"\n",
    "sql = f\"SELECT alloydb_ai_nl.g_create_configuration( '{nl_config}' );\"\n",
    "await run_alloydb_query(finance_db_pool, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795771c7-c598-4418-a045-061a1037583e",
   "metadata": {},
   "source": [
    "### Register Tables to the `finance_agent_config` Config\n",
    "\n",
    "This step tells our natural language configuration which tables it should be aware of when generating SQL queries. Success will be shown as `g_manage_configuration = True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae870bc1-0b25-4ea8-8122-646247748b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nl_config = \"finance_agent_config\"\n",
    "sql = f\"\"\"\n",
    "SELECT alloydb_ai_nl.g_manage_configuration(\n",
    "    operation => 'register_table_view',\n",
    "    configuration_id_in => '{nl_config}',\n",
    "    table_views_in=>'{{public.transactions, public.cards, public.users, public.mcc_codes, public.fraud_labels}}'\n",
    ");\n",
    "\"\"\"\n",
    "print(sql)\n",
    "await run_alloydb_query(finance_db_pool, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb883a1b-fb1d-4d1e-863d-a5c7cd10415b",
   "metadata": {},
   "source": [
    "### Create and Apply Context for Table and Columns\n",
    "\n",
    "Here, we automatically generate descriptions for our tables and columns. This context is crucial for helping the AI model understand the meaning of our schema. Success will be shown as `generate_schema_context = None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b78623-4df2-40fc-9ffa-fea8f3924cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate schema contexts for the tables and their columns that are registered in the nla_demo_cfg configuration\n",
    "# This query opulates the alloydb_ai_nl.generated_schema_context_view view with context. Passing TRUE overwrites \n",
    "# the context in this view from previous runs.\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT alloydb_ai_nl.generate_schema_context(\n",
    "  '{nl_config}',\n",
    "  TRUE\n",
    ");\n",
    "\"\"\"\n",
    "await run_alloydb_query(finance_db_pool, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e076f88-799f-40d7-abd6-cec05e482ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the generated context\n",
    "sql = f\"\"\"\n",
    "SELECT *\n",
    "FROM alloydb_ai_nl.generated_schema_context_view;\n",
    "\"\"\"\n",
    "await run_alloydb_query(finance_db_pool, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0db0f-e5c2-4c10-be24-99478d9e8e17",
   "metadata": {},
   "source": [
    "You can optionally modify the generated context using commands like the following:\n",
    "\n",
    "```\n",
    "SELECT alloydb_ai_nl.update_generated_relation_context(\n",
    "  'nla_demo.products',\n",
    "  'The \"nla_demo.products\" table stores product details such as ID, name, description, brand, category linkage, and record creation time.'\n",
    ");\n",
    "\n",
    "SELECT alloydb_ai_nl.update_generated_column_context(\n",
    "  'nla_demo.products.name',\n",
    "  'The \"name\" column in the \"nla_demo.products\" table contains the specific name or title of each product.'\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf2970-2b36-4477-85db-ff0123298aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the generated context\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT *\n",
    "FROM alloydb_ai_nl.generated_schema_context_view;\n",
    "\"\"\"\n",
    "result = await run_alloydb_query(finance_db_pool, sql)\n",
    "\n",
    "for index, row in result.iterrows():\n",
    "    sql = f\"\"\"\n",
    "    SELECT alloydb_ai_nl.apply_generated_column_context(\n",
    "      '{row['schema_object']}', true\n",
    "    );\n",
    "    \"\"\"\n",
    "    result = await run_alloydb_query(finance_db_pool, sql)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f86327-a1e0-45d4-8dcf-52705af2a66f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify context has all been applied - should return an empty result\n",
    "sql = f\"\"\"\n",
    "SELECT *\n",
    "FROM alloydb_ai_nl.generated_schema_context_view;\n",
    "\"\"\"\n",
    "await run_alloydb_query(finance_db_pool, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332aaa34-2a27-4042-bd08-53a2918e726c",
   "metadata": {},
   "source": [
    "### Define Query Templates\n",
    "\n",
    "Query templates provide the model with examples of well-formed SQL and natural language queries, which improves the accuracy of the generated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896b270-5fe8-44b6-bb48-f49409fd931c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_array = []\n",
    "\n",
    "sql_array.append(f\"\"\"\n",
    "SELECT alloydb_ai_nl.add_template(\n",
    "    nl_config_id => '{nl_config}',\n",
    "    intent => 'Are there any recent transactions where a chip card was swiped?',\n",
    "    sql => 'SELECT t.id AS transaction_id, t.date, t.amount, c.card_number, u.id AS user_id, t.merchant_city, t.merchant_state FROM transactions t JOIN cards c ON t.card_id = c.id JOIN users u ON t.client_id = u.id WHERE c.has_chip = TRUE AND t.use_chip <> ''Chip''',\n",
    "    sql_explanation => 'This query identifies transactions where a card equipped with a chip was used in a way that didn''t utilize the chip (e.g. swiped). This can be an indicator of card skimming or cloning, as fraudsters may create a magnetic stripe copy of a chip card.'\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "sql_array.append(f\"\"\"\n",
    "SELECT alloydb_ai_nl.add_template(\n",
    "    nl_config_id => '{nl_config}',\n",
    "    intent => 'Show me recent occurrences of high value transactions for customers with low income.',\n",
    "    sql => 'SELECT u.id AS user_id, u.yearly_income, t.id AS transaction_id, t.amount, t.date, mc.description AS merchant_category FROM users u JOIN transactions t ON u.id = t.client_id JOIN mcc_codes mc ON t.mcc = mc.mcc WHERE u.yearly_income < ''30000'' AND t.amount > 1000.00 ORDER BY t.amount DESC',\n",
    "    sql_explanation => 'This query looks for transactions that are significantly larger than what might be expected from a user''s reported income. This could indicate account takeover or identity theft. For this example, we could define a high-value transaction as one over $1,000 for users with a yearly income of less than $30,000.'\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "sql_array.append(f\"\"\"\n",
    "SELECT alloydb_ai_nl.add_template(\n",
    "    nl_config_id => '{nl_config}',\n",
    "    intent => 'Show me transactions that could not feasibly occur so quickly in different locations.',\n",
    "    sql => 'WITH RankedTransactions AS (SELECT client_id, id AS transaction_id, date, merchant_city, LAG(date, 1) OVER (PARTITION BY client_id ORDER BY date) AS previous_transaction_date, LAG(merchant_city, 1) OVER (PARTITION BY client_id ORDER BY date) AS previous_merchant_city FROM transactions) SELECT rt.client_id, rt.transaction_id, rt.date, rt.merchant_city, rt.previous_transaction_date, rt.previous_merchant_city FROM RankedTransactions rt WHERE rt.previous_transaction_date IS NOT NULL AND rt.merchant_city <> rt.previous_merchant_city AND rt.date - rt.previous_transaction_date < INTERVAL ''1 hour'' ORDER BY rt.client_id, rt.date',\n",
    "    sql_explanation => 'This query identifies multiple transactions for the same user that occur in different cities within a short time frame (e.g. one hour). This is a strong indicator of card-not-present fraud or that the card has been stolen or cloned.'\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "sql_array.append(f\"\"\"\n",
    "SELECT alloydb_ai_nl.add_template(\n",
    "    nl_config_id => '{nl_config}',\n",
    "    intent => 'Which types of transactions are most associated with fraud?',\n",
    "    sql => 'SELECT mc.description, COUNT(t.id) AS fraudulent_transaction_count FROM transactions t JOIN fraud_labels fl ON t.id = fl.transaction_id JOIN mcc_codes mc ON t.mcc = mc.mcc WHERE fl.is_fraud = TRUE GROUP BY mc.description ORDER BY fraudulent_transaction_count DESC LIMIT 10',\n",
    "    sql_explanation => 'Understanding which types of merchants are most frequently associated with fraudulent transactions can help in developing rules for fraud detection systems. This query counts the number of fraudulent transactions for each Merchant Category Code (MCC).'\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "for sql in sql_array:\n",
    "    result = await run_alloydb_query(finance_db_pool, sql)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79554df1-240b-4633-b869-fdea9ad4fb1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View Created Templates\n",
    "sql = f\"\"\"\n",
    "SELECT id, nl, sql, intent, psql, pintent\n",
    "FROM alloydb_ai_nl.template_store_view\n",
    "WHERE config = '{nl_config}'\n",
    "\"\"\"\n",
    "await run_alloydb_query(finance_db_pool, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90533be9-a51c-47f7-8081-b9ec259873af",
   "metadata": {},
   "source": [
    "You can optionally drop examples with a query like the following (pass the template ID as the only parameter):\n",
    "\n",
    "```\n",
    "SELECT alloydb_ai_nl.drop_template(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96bb4e-0c60-471d-bf1c-df795d9b9b47",
   "metadata": {},
   "source": [
    "## Generate and Execute SQL with AlloyDB Natural Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d323f22-e79a-4ba8-acfd-603f1afa75ba",
   "metadata": {},
   "source": [
    "### Generate SQL from Natural Language Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec5a077-d403-44a9-885a-363bf635dbad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"SELECT\n",
    "    alloydb_ai_nl.get_sql(\n",
    "        '{nl_config}',\n",
    "        'What is the total amount of transactions from the last 10 years?'\n",
    "    ) ->> 'sql';\"\"\"\n",
    "result = await run_alloydb_query(finance_db_pool, sql, output_as_df = False)\n",
    "print(result.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf7d9f-869e-47b2-b882-3ce50fb42bd5",
   "metadata": {},
   "source": [
    "### Execute SQL from Natural Language Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14edbc-c585-4c6e-9825-015cf744cbea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"SELECT\n",
    "    alloydb_ai_nl.execute_nl_query(\n",
    "        'What is the total amount of transactions from the last 10 years?',\n",
    "        '{nl_config}'\n",
    "    )\"\"\"\n",
    "await run_alloydb_query(finance_db_pool, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7101b-3dcf-42aa-9b36-0036003c5d45",
   "metadata": {},
   "source": [
    "## Use AlloyDB Natural Language with MCP Toolbox\n",
    "\n",
    "MCP Toolbox has [built-in](https://googleapis.github.io/genai-toolbox/resources/tools/alloydbainl/alloydb-ai-nl/) support for AlloyDB's Natural Language feature. To demonstrate the flexibility of this feature, let's deploy a 1-tool version of our Toolbox instance and ask a number of questions about the transactions, cards, users, mcc codes, and fraud labels in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67fedf-d512-4120-9da1-3ac0cfdffa86",
   "metadata": {},
   "source": [
    "### Update the `tools.yaml` File\n",
    "\n",
    "The `alloydb-ai-nl` tool type allows the agent to use the AlloyDB Natural Language feature to convert natural language questions into SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f43f61-cf60-4199-bc7d-98da6ee88ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://googleapis.github.io/genai-toolbox/resources/sources/spanner/\n",
    "#            https://googleapis.github.io/genai-toolbox/resources/tools/\n",
    "#            https://googleapis.github.io/genai-toolbox/resources/tools/spanner-sql/\n",
    "#            https://googleapis.github.io/genai-toolbox/resources/sources/alloydb-pg/\n",
    "#            https://googleapis.github.io/genai-toolbox/resources/tools/postgres-sql/\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "tools_config = {\n",
    "  \"sources\": {\n",
    "    \"alloydb-finance-source\": {\n",
    "      \"kind\": \"alloydb-postgres\",\n",
    "      \"project\": f\"{project_id}\",\n",
    "      \"region\": f\"{region}\",\n",
    "      \"cluster\": f\"{alloydb_cluster}\",\n",
    "      \"instance\": f\"{alloydb_instance}\",\n",
    "      \"database\": f\"{alloydb_database}\",\n",
    "      \"user\": \"postgres\",\n",
    "      \"password\": \"${ALLOYDB_PASSWORD}\",\n",
    "      \"ipType\": \"private\"\n",
    "    }\n",
    "  },\n",
    "  \"tools\": {\n",
    "    \"get_finance_database_context\": {\n",
    "      \"kind\": \"alloydb-ai-nl\",\n",
    "      \"source\": \"alloydb-finance-source\",\n",
    "      \"description\": \"Use this tool to look up information about financial transactions, credit cards, customers, mcc codes, and historical fraud labels.\",\n",
    "      \"nlConfig\": f\"{nl_config}\"\n",
    "    }\n",
    "  },\n",
    "  \"toolsets\": {\n",
    "    \"finance-toolset\": [\n",
    "      \"get_finance_database_context\",\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(\"tools.yaml\", \"w\") as file:\n",
    "    file.write(json.dumps(tools_config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd4969-3d6f-4921-bbf9-1b0f5899ea97",
   "metadata": {},
   "source": [
    "### Write Updated `tools.yaml` to Secret Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999df86e-a93d-4906-92a6-73e6c89a41f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the secret\n",
    "! gcloud secrets versions add tools --data-file=tools.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92096cdc-a5c9-4da6-adb1-855f9258e984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up the local file\n",
    "import os\n",
    "os.remove('tools.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a826d2a6-37a2-4955-89c7-f5ecf66729e9",
   "metadata": {},
   "source": [
    "### Update Toolbox with New `tools.yaml` File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968173fa-5553-489e-a4cf-de1786d615a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://cloud.google.com/sdk/gcloud/reference/run/services/update\n",
    "\n",
    "! gcloud run services update toolbox --no-user-output-enabled \\\n",
    "    --update-secrets=\"/app/tools.yaml=tools:latest,ALLOYDB_PASSWORD=alloydb-password:latest\" \\\n",
    "    --region={region}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d2c6c-6313-4e8e-b7cb-b0db15812cbd",
   "metadata": {},
   "source": [
    "### Execute Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1868f897-467a-4934-8846-c01b1664708b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a list of natural language questions to ask the MCP Toolbox tool.\n",
    "nl_questions = [\n",
    "    \"How many users are in the database?\",\n",
    "    \"Show me the most recent transactions for customer 123\",\n",
    "    \"Tell me about the most recent fraudulent transactions\",\n",
    "    \"Which customer has the most credit cards, and how many do they have?\",\n",
    "    \"What are the top 10 highest salaries for our customers?\",\n",
    "    \"Which mcc codes are most commonly associated with fraud?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205af69-d0bf-4734-84d5-844468f72430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from toolbox_core import ToolboxClient, auth_methods\n",
    "import urllib\n",
    "\n",
    "import google.auth.transport.requests\n",
    "import google.oauth2.id_token\n",
    "\n",
    "\n",
    "def get_auth_token(endpoint):\n",
    "    # Cloud Run uses your service's hostname as the `audience` value\n",
    "    # audience = 'https://my-cloud-run-service.run.app/'\n",
    "    # For Cloud Run, `endpoint` is the URL (hostname + path) receiving the request\n",
    "    # endpoint = 'https://my-cloud-run-service.run.app/my/awesome/url'\n",
    "    \n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    id_token = google.oauth2.id_token.fetch_id_token(auth_req, endpoint)\n",
    "\n",
    "    return id_token\n",
    "\n",
    "# Get endpoint\n",
    "toolbox_url = ! gcloud run services describe toolbox --region {region} --format 'value(metadata.annotations.\"run.googleapis.com/urls\")'\n",
    "toolbox_url = json.loads(toolbox_url[0])[0]\n",
    "print(f\"Toolbox Cloud Run endpoint: {toolbox_url}\")\n",
    "\n",
    "# Get auth_token\n",
    "auth_token = get_auth_token(toolbox_url)\n",
    "auth_token_provider = auth_methods.aget_google_id_token # can also use sync method\n",
    "\n",
    "# Run tools \n",
    "async with ToolboxClient(\n",
    "    toolbox_url,\n",
    "    client_headers={\"Authorization\": f\"Bearer {auth_token}\"},\n",
    ") as client:\n",
    "    tools = await client.load_toolset(\"finance-toolset\")\n",
    "    for t in tools:\n",
    "        print(f\"\\nRunning tool: {t._ToolboxTool__url}\")\n",
    "        for q in nl_questions:\n",
    "            print(f\"Question: {q}\")\n",
    "            result = await t(q)\n",
    "            json_result = json.loads(result)\n",
    "            print(\"Tool result:\\n\")\n",
    "            print(json.dumps(json_result, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119b9c7-baa4-4a62-b187-6df8936293b6",
   "metadata": {},
   "source": [
    "## Use AlloyDB Natural Language with the ADK Agent\n",
    "\n",
    "Now let's see our Finance Agent in action with this new flexible and powerful semantic query tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3128816-c74f-4b09-8953-41a3a33319dd",
   "metadata": {},
   "source": [
    "### Update the Agent with New Tools\n",
    "\n",
    "Updating the TOOLBOX_URL environment variable will force the ADK Agent to grab the new Tool we defined in the previous steps. All other environment variables will remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69807d5-0746-4ec1-9324-6e92ac2fd261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud run services update finance-agent --no-user-output-enabled \\\n",
    "    --update-env-vars=TOOLBOX_URL={toolbox_url} \\\n",
    "    --region={region}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc82e9-ba39-4ee5-a220-ad7ca1c3ac50",
   "metadata": {},
   "source": [
    "### Define Functions for Agent Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad8e31-caad-4f5f-999a-f51f0c08db4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import uuid\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import google.oauth2.id_token\n",
    "\n",
    "# --- Session Tracking ---\n",
    "# Use a set to keep track of sessions created during this notebook's execution.\n",
    "created_sessions = set()\n",
    "\n",
    "def create_agent_session(agent_url: str, auth_token: str, app_name: str, user_id: str, session_id: str) -> bool:\n",
    "    \"\"\"\n",
    "    Explicitly creates a new session for the agent.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    session_url = f\"{agent_url}/apps/{app_name}/users/{user_id}/sessions/{session_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {auth_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\"state\": {}}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(session_url, headers=headers, json=payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Successfully created session: {session_id}\")\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"\\nError creating session: {e}\")\n",
    "        if e.response is not None:\n",
    "            print(f\"Response Status: {e.response.status_code}\")\n",
    "            print(f\"Response Text: {e.response.text}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def invoke_agent_streaming(query: str, session_id: str = None, user_id: str = 'default-user') -> None:\n",
    "    \"\"\"\n",
    "    Invokes the agent on Cloud Run, creating a session only if it's new.\n",
    "    \"\"\"\n",
    "    if session_id is None:\n",
    "        session_id = str(uuid.uuid4())\n",
    "    \n",
    "    app_name = \"finance_agent\"\n",
    "\n",
    "    try:\n",
    "        agent_url = !gcloud run services describe finance-agent --region {region} --format 'value(status.url)'\n",
    "        agent_url = agent_url[0]\n",
    "        agent_auth_token = get_auth_token(agent_url)\n",
    "\n",
    "        # --- Step 1: Create Session ONLY IF IT'S NEW ---\n",
    "        if session_id not in created_sessions:\n",
    "            if create_agent_session(agent_url, agent_auth_token, app_name, user_id, session_id):\n",
    "                # Add the new session_id to our set of tracked sessions\n",
    "                created_sessions.add(session_id)\n",
    "            else:\n",
    "                return # Stop if session creation fails\n",
    "\n",
    "        # --- Step 2: Send the Message ---\n",
    "        invoke_url = f\"{agent_url}/run_sse\"\n",
    "        headers = {\n",
    "            \"Accept\": \"text/event-stream\",\n",
    "            \"Authorization\": f\"Bearer {agent_auth_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"app_name\": app_name,\n",
    "            \"session_id\": session_id,\n",
    "            \"user_id\": user_id,\n",
    "            \"new_message\": {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [{\"text\": query}]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with requests.post(invoke_url, headers=headers, json=payload, stream=True, timeout=300) as response:\n",
    "            response.raise_for_status()\n",
    "            print(f\"\\nUser Query: {query}\")\n",
    "            print(\"Agent Response:\")\n",
    "            for line in response.iter_lines():\n",
    "                if line and line.decode('utf-8').startswith('data: '):\n",
    "                    try:\n",
    "                        json_data = json.loads(line.decode('utf-8')[6:])\n",
    "                        text = json_data.get('content', {}).get('parts', [{}])[0].get('text', '')\n",
    "                        print(text, end='', flush=True)\n",
    "                    except (json.JSONDecodeError, IndexError):\n",
    "                        continue\n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"\\nError invoking agent: {e}\")\n",
    "        if e.response is not None:\n",
    "            print(f\"Response Status: {e.response.status_code}\")\n",
    "            print(f\"Response Text: {e.response.text}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6b346-9f2c-457e-974f-1f05020821ae",
   "metadata": {},
   "source": [
    "### Query the Agent\n",
    "\n",
    "We'll execute a multi-turn conversation with different natural language questions to watch how our ADK Agent uses the new AlloyDB Natural Language tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d1e08-a968-4954-858f-bea6c19f3c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nl_questions = [\n",
    "    \"How many users are in the database?\",\n",
    "    \"Show me the most recent transactions for customer 123\",\n",
    "    \"Tell me about the most recent fraudulent transactions\",\n",
    "    \"Which customer has the most credit cards, and how many do they have?\",\n",
    "    \"What are the top 10 highest salaries for our customers?\",\n",
    "    \"Which mcc codes are most commonly associated with fraud?\"\n",
    "]\n",
    "\n",
    "# Set a unique ID for the conversation and user\n",
    "conversation_session_id = f\"session_{uuid.uuid4()}\"\n",
    "conversation_user_id = f\"user_{uuid.uuid4()}\"\n",
    "\n",
    "for q in nl_questions:\n",
    "\n",
    "    # The first call will see the session_id is new, create it, and add it to the 'created_sessions' set.\n",
    "    invoke_agent_streaming(\n",
    "        q,\n",
    "        session_id=conversation_session_id,\n",
    "        user_id=conversation_user_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd698965",
   "metadata": {},
   "source": [
    "Congratulations, you have completed Module 4! Proceed to [`5_build_an_interactive_finance_agent.ipynb`](./5_build_an_interactive_finance_agent.ipynb) to leverage all of the components you've built so far to build a power interactive ADK agent to assist Fraud Analysts with their fraud investigations."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
