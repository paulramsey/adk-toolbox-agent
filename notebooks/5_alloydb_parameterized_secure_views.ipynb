{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321adc7f-f386-4dbf-bb9c-d5f789421f78",
   "metadata": {},
   "source": [
    "# Setup and AlloyDB Parameterized Secure Views\n",
    "\n",
    "Parameterized Secure Views (PSV) in AlloyDB allow you to define tables and columns that natural-language queries can pull data from, and add additional restrictions on the range of rows available to an individual application user. These restrictions let you tightly control the data that your application users can view through natural-language queries, no matter how your users phrase these queries. PSVs are designed to protect against prompt injection attacks and help ensure that end users can view only the data that they are supposed to access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927ae02-125e-4d3e-a885-1e437e6bddb6",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842a3ca3-4f62-4190-bbb6-10e7da9b13c8",
   "metadata": {},
   "source": [
    "### Define Notebook Variables\n",
    "\n",
    "Update the variables below to match your environment. You will be prompted for the AlloyDB password you chose then you provisioned the environment with Terraform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa431b-2efd-4db1-89e1-b97b4fc722df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project variables\n",
    "project_id = \"your-project\"\n",
    "region = \"your-region\"\n",
    "vpc = \"demo-vpc\"\n",
    "gcs_bucket_name = f\"project-files-{project_id}\"\n",
    "\n",
    "# AlloyDB variables\n",
    "alloydb_cluster = \"my-alloydb-cluster\"\n",
    "alloydb_instance = \"my-alloydb-instance\"\n",
    "alloydb_database = \"finance\"\n",
    "alloydb_password = input(\"Please enter the password for the AlloyDB 'postgres' database user: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db8a8b-cae4-4978-bb9a-e2e5a8465696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set env variable to suppress annoying system warnings when running shell commands\n",
    "%env GRPC_ENABLE_FORK_SUPPORT=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc39dc-f6ee-455e-853a-643c12918c2f",
   "metadata": {},
   "source": [
    "### Connect to your Google Cloud Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d972baa-ffa7-47c7-a965-fdaa114262b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure gcloud.\n",
    "!gcloud config set project {project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a124be-854e-4465-bab2-44ea855d9256",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66056e25-5e3a-47e3-a842-8fe6fadd9a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure the root logger to output messages with INFO level or above\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='%(asctime)s[%(levelname)5s][%(name)14s] - %(message)s',  datefmt='%H:%M:%S', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72eb606-f634-424f-be79-c1b75b1b1e14",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2e387-9dfc-49c2-8a32-2c54d7eebcc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --quiet google-cloud-storage==2.19.0 \\\n",
    "                      asyncpg==0.30.0 \\\n",
    "                      google.cloud.alloydb.connector==1.9.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5cf49a-a04d-429c-a5f3-2df8e068fc81",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349aa29e-ccbb-4aef-b51b-20d9130c1e53",
   "metadata": {},
   "source": [
    "#### REST API Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0ad2b-fed0-434f-baa7-69247ee476c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import google.auth\n",
    "import json\n",
    "\n",
    "# Get an access token based upon the current user\n",
    "creds, _ = google.auth.default()\n",
    "authed_session = google.auth.transport.requests.AuthorizedSession(creds)\n",
    "access_token=creds.token\n",
    "\n",
    "if project_id:\n",
    "  authed_session.headers.update({\"x-goog-user-project\": project_id}) # Required to workaround a project quota bug\n",
    "\n",
    "def rest_api_helper(\n",
    "    url: str,\n",
    "    http_verb: str,\n",
    "    request_body: dict = None,\n",
    "    params: dict = None,\n",
    "    session: requests.Session = authed_session,\n",
    "  ) -> dict:\n",
    "  \"\"\"Calls a REST API using a pre-authenticated requests Session.\"\"\"\n",
    "\n",
    "  headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "  try:\n",
    "\n",
    "    if http_verb == \"GET\":\n",
    "      response = session.get(url, headers=headers, params=params)\n",
    "    elif http_verb == \"POST\":\n",
    "      response = session.post(url, json=request_body, headers=headers, params=params)\n",
    "    elif http_verb == \"PUT\":\n",
    "      response = session.put(url, json=request_body, headers=headers, params=params)\n",
    "    elif http_verb == \"PATCH\":\n",
    "      response = session.patch(url, json=request_body, headers=headers, params=params)\n",
    "    elif http_verb == \"DELETE\":\n",
    "      response = session.delete(url, headers=headers, params=params)\n",
    "    else:\n",
    "      raise ValueError(f\"Unknown HTTP verb: {http_verb}\")\n",
    "\n",
    "    # Raise an exception for bad status codes (4xx or 5xx)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Check if response has content before trying to parse JSON\n",
    "    if response.content:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {} # Return empty dict for empty responses (like 204 No Content)\n",
    "\n",
    "  except requests.exceptions.RequestException as e:\n",
    "      # Catch potential requests library errors (network, timeout, etc.)\n",
    "      # Log detailed error information\n",
    "      print(f\"Request failed: {e}\")\n",
    "      if e.response is not None:\n",
    "          print(f\"Request URL: {e.request.url}\")\n",
    "          print(f\"Request Headers: {e.request.headers}\")\n",
    "          print(f\"Request Body: {e.request.body}\")\n",
    "          print(f\"Response Status: {e.response.status_code}\")\n",
    "          print(f\"Response Text: {e.response.text}\")\n",
    "          # Re-raise a more specific error or a custom one\n",
    "          raise RuntimeError(f\"API call failed with status {e.response.status_code}: {e.response.text}\") from e\n",
    "      else:\n",
    "          raise RuntimeError(f\"API call failed: {e}\") from e\n",
    "  except json.JSONDecodeError as e:\n",
    "      print(f\"Failed to decode JSON response: {e}\")\n",
    "      print(f\"Response Text: {response.text}\")\n",
    "      raise RuntimeError(f\"Invalid JSON received from API: {response.text}\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f4658-e4c5-4c3f-badf-da3f3d6cdd3a",
   "metadata": {},
   "source": [
    "#### AlloyDB Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b976a5-7b18-41d2-89c9-dbd2b9a2aa45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create AlloyDB Query Helper Function\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text, exc\n",
    "import pandas as pd\n",
    "\n",
    "async def run_alloydb_query(pool, sql: str, params = None, output_as_df: bool = True):\n",
    "    \"\"\"Executes a SQL query or statement against the database pool.\n",
    "\n",
    "    Handles various SQL statements:\n",
    "    - SELECT/WITH: Returns results as a DataFrame (if output_as_df=True)\n",
    "      or ResultProxy. Supports parameters. Does not commit.\n",
    "    - EXPLAIN/EXPLAIN ANALYZE: Executes the explain, returns the query plan\n",
    "      as a formatted multi-line string. Ignores output_as_df.\n",
    "      Supports parameters. Does not commit.\n",
    "    - INSERT/UPDATE/DELETE/CREATE/ALTER etc.: Executes the statement,\n",
    "      commits the transaction, logs info, and returns the ResultProxy.\n",
    "      Supports single or bulk parameters (executemany).\n",
    "\n",
    "    Args:\n",
    "      pool: An asynchronous SQLAlchemy connection pool.\n",
    "      sql: A string containing the SQL query or statement template.\n",
    "      params: Optional.\n",
    "        - None: Execute raw SQL (Use with caution for non-SELECT/EXPLAIN).\n",
    "        - dict or tuple: Parameters for a single execution.\n",
    "        - list of dicts/tuples: Parameters for bulk execution (executemany).\n",
    "      output_as_df (bool): If True and query is SELECT/WITH, return pandas DataFrame.\n",
    "                           Ignored for EXPLAIN and non-data-returning statements.\n",
    "\n",
    "    Returns:\n",
    "      pandas.DataFrame | str | sqlalchemy.engine.Result | None:\n",
    "        - DataFrame: For SELECT/WITH if output_as_df=True.\n",
    "        - str: For EXPLAIN/EXPLAIN ANALYZE, containing the formatted query plan.\n",
    "        - ResultProxy: For non-SELECT/WITH/EXPLAIN statements, or SELECT/WITH\n",
    "                       if output_as_df=False.\n",
    "        - None: If a SQLAlchemy ProgrammingError or other specific error occurs.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Catches and logs `sqlalchemy.exc.ProgrammingError`, returning None.\n",
    "                   May re-raise other database exceptions.\n",
    "\n",
    "    Example Execution:\n",
    "      Single SELECT:\n",
    "        sql_select = \"SELECT ticker, company_name from investments LIMIT 5\"\n",
    "        df_result = await run_alloydb_query(pool, sql_select)\n",
    "\n",
    "      Single non-SELECT - Parameterized (Safe!):\n",
    "        Parameterized INSERT:\n",
    "          sql_insert = \"INSERT INTO investments (ticker, company_name) VALUES (:ticker, :name)\"\n",
    "          params_insert = {\"ticker\": \"NEW\", \"name\": \"New Company\"}\n",
    "          insert_result = await run_alloydb_query(pool, sql_insert, params_insert)\n",
    "\n",
    "        Parameterized UPDATE:\n",
    "          sql_update = \"UPDATE products SET price = :price WHERE id = :product_id\"\n",
    "          params_update = {\"price\": 99.99, \"product_id\": 123}\n",
    "          update_result = await run_alloydb_query(pool, sql_update, params_update)\n",
    "\n",
    "      Bulk Update:\n",
    "        docs = pd.DataFrame([\n",
    "            {'id': 101, 'sparse_embedding': '[0.1, 0.2]'},\n",
    "            {'id': 102, 'sparse_embedding': '[0.3, 0.4]'},\n",
    "            # ... more rows\n",
    "        ])\n",
    "\n",
    "        update_sql_template = '''\n",
    "            UPDATE products\n",
    "            SET sparse_embedding = :embedding,\n",
    "                sparse_embedding_model = 'BM25'\n",
    "            WHERE id = :product_id\n",
    "        ''' # Using named parameters :param_name\n",
    "\n",
    "        # Prepare list of dictionaries for params\n",
    "        data_to_update = [\n",
    "            {\"embedding\": row.sparse_embedding, \"product_id\": row.id}\n",
    "            for row in docs.itertuples(index=False)\n",
    "        ]\n",
    "\n",
    "        if data_to_update:\n",
    "          bulk_result = await run_alloydb_query(pool, update_sql_template, data_to_update)\n",
    "          # bulk_result is the SQLAlchemy ResultProxy\n",
    "\n",
    "    \"\"\"\n",
    "    sql_lower_stripped = sql.strip().lower()\n",
    "    is_select_with = sql_lower_stripped.startswith(('select', 'with'))\n",
    "    is_explain = sql_lower_stripped.startswith('explain')\n",
    "\n",
    "    # Determine if the statement is expected to return data rows or a plan\n",
    "    is_data_returning = is_select_with or is_explain\n",
    "\n",
    "    # Determine actual DataFrame output eligibility (only for SELECT/WITH)\n",
    "    effective_output_as_df = output_as_df and is_select_with\n",
    "\n",
    "    # Check if params suggest a bulk operation (for logging purposes)\n",
    "    is_bulk_operation = isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], (dict, tuple, list))\n",
    "\n",
    "    async with pool.connect() as conn:\n",
    "        try:\n",
    "          # Execute with or without params\n",
    "          if params:\n",
    "              result = await conn.execute(text(sql), params)\n",
    "          else:\n",
    "              # Add warning for raw SQL only if it's NOT data-returning\n",
    "              #if not is_data_returning:\n",
    "                  #logging.warning(\"Executing non-SELECT/EXPLAIN raw SQL without parameters. Ensure SQL is safe.\")\n",
    "              result = await conn.execute(text(sql))\n",
    "\n",
    "          # --- Handle statements that return data or plan ---\n",
    "          if is_data_returning:\n",
    "              if is_explain:\n",
    "                  # Fetch and format EXPLAIN output as a string\n",
    "                    try:\n",
    "                        plan_rows = result.fetchall()\n",
    "                        # EXPLAIN output is usually text in the first column\n",
    "                        query_plan = \"\\n\".join([str(row[0]) for row in plan_rows])\n",
    "                        #logging.info(f\"EXPLAIN executed successfully for: {sql[:100]}...\")\n",
    "                        return query_plan\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error fetching/formatting EXPLAIN result: {e}\")\n",
    "                        return None\n",
    "              else: # Handle SELECT / WITH\n",
    "                  if effective_output_as_df:\n",
    "                      try:\n",
    "                          rows = result.fetchall()\n",
    "                          column_names = result.keys()\n",
    "                          df = pd.DataFrame(rows, columns=column_names)\n",
    "                          #logging.info(f\"SELECT/WITH executed successfully, returning DataFrame for: {sql[:100]}...\")\n",
    "                          return df\n",
    "                      except Exception as e:\n",
    "                          logging.error(f\"Error converting SELECT result to DataFrame: {e}\")\n",
    "                          logging.info(f\"Returning raw ResultProxy for SELECT/WITH due to DataFrame conversion error for: {sql[:100]}...\")\n",
    "                          return result # Fallback to raw result\n",
    "                  else:\n",
    "                      # Return raw result proxy for SELECT/WITH if df output not requested\n",
    "                      #logging.info(f\"SELECT/WITH executed successfully, returning ResultProxy for: {sql[:100]}...\")\n",
    "                      return result\n",
    "\n",
    "          # --- Handle Non-Data Returning Statements (INSERT, UPDATE, DELETE, CREATE, etc.) ---\n",
    "          else:\n",
    "              await conn.commit() # Commit changes ONLY for these statements\n",
    "              operation_type = sql.strip().split()[0].upper()\n",
    "              row_count = result.rowcount # Note: rowcount behavior varies\n",
    "\n",
    "              if is_bulk_operation:\n",
    "                  print(f\"Bulk {operation_type} executed for {len(params)} items. Result rowcount: {row_count}\")\n",
    "              elif operation_type in ['INSERT', 'UPDATE', 'DELETE']:\n",
    "                  print(f\"{operation_type} statement executed successfully. {row_count} row(s) affected.\")\n",
    "              else: # CREATE, ALTER, etc.\n",
    "                  print(f\"{operation_type} statement executed successfully. Result rowcount: {row_count}\")\n",
    "              return result # Return the result proxy\n",
    "\n",
    "        except exc.ProgrammingError as e:\n",
    "            # Log the error with context\n",
    "            logging.error(f\"SQL Programming Error executing query:\\nSQL: {sql[:500]}...\\nParams (sample): {str(params)[:500]}...\\nError: {e}\")\n",
    "            # Rollback might happen automatically on context exit with error, but explicit can be clearer\n",
    "            # await conn.rollback() # Consider if needed based on pool/transaction settings\n",
    "            return None # Return None on handled programming errors\n",
    "        except Exception as e:\n",
    "            # Log other unexpected errors\n",
    "            logging.error(f\"An unexpected error occurred during query execution:\\nSQL: {sql[:500]}...\\nError: {e}\")\n",
    "            # await conn.rollback() # Consider if needed\n",
    "            raise # Re-raise unexpected errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c9f67-ab0a-419f-bb2e-2ccb3b94be33",
   "metadata": {},
   "source": [
    "## Setup Parameterized Secure Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989a7a1-7fd3-4fa4-9782-a69029594a71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to the AlloyDB Cluster\n",
    "\n",
    "This function will create a connection pool to your AlloyDB instance using the AlloyDB Python connector. The AlloyDB Python connector will automatically create secure connections to your AlloyDB instance using mTLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8c550-32a1-4bab-b1ed-86b85ea92252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncpg\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.asyncio import AsyncEngine, create_async_engine\n",
    "\n",
    "from google.cloud.alloydb.connector import AsyncConnector, IPTypes\n",
    "\n",
    "async def init_connection_pool(connector: AsyncConnector, db_name: str = alloydb_database, pool_size: int = 5) -> AsyncEngine:\n",
    "    # initialize Connector object for connections to AlloyDB\n",
    "    connection_string = f\"projects/{project_id}/locations/{region}/clusters/{alloydb_cluster}/instances/{alloydb_instance}\"\n",
    "\n",
    "    async def getconn() -> asyncpg.Connection:\n",
    "        conn: asyncpg.Connection = await connector.connect(\n",
    "            connection_string,\n",
    "            \"asyncpg\",\n",
    "            user=\"postgres\",\n",
    "            password=alloydb_password,\n",
    "            db=db_name,\n",
    "            ip_type=IPTypes.PRIVATE, # Optionally use IPTypes.PUBLIC\n",
    "        )\n",
    "        return conn\n",
    "\n",
    "    pool = create_async_engine(\n",
    "        \"postgresql+asyncpg://\",\n",
    "        async_creator=getconn,\n",
    "        pool_size=pool_size,\n",
    "        max_overflow=0,\n",
    "        isolation_level='AUTOCOMMIT'\n",
    "    )\n",
    "    return pool\n",
    "\n",
    "connector = AsyncConnector()\n",
    "\n",
    "finance_db_pool = await init_connection_pool(connector, f\"{alloydb_database}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b46ab-243f-4693-a86d-0090c4c4cdeb",
   "metadata": {},
   "source": [
    "### Create `parameterized_views` Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4e06f-4bc6-45da-84ae-9c58f7611c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"CREATE EXTENSION IF NOT EXISTS parameterized_views;\"\n",
    "await run_alloydb_query(finance_db_pool, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03723525-5e86-4b14-b24b-8d6d96ddb289",
   "metadata": {},
   "source": [
    "### Create a Parameterized Secure View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdcf4a-a326-4f3e-ae29-654381d0ad90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql = \"SELECT extversion FROM pg_extension WHERE extname = 'alloydb_ai_nl';\"\n",
    "await run_alloydb_query(postgres_db_pool, sql)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
